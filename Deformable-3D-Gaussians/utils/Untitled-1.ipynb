{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pretrained Stable Diffusion model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Assume encoded_image1 and encoded_image2 are of shape (1, 197, 768)\n",
    "encoded_image1 = torch.randn(1, 197, 768).cuda()\n",
    "encoded_image2 = torch.randn(1, 197, 768).cuda()\n",
    "\n",
    "# Concatenate the encoded images\n",
    "combined_encoded_images = torch.cat((encoded_image1, encoded_image2), dim=1)  # shape: (1, 394, 768)\n",
    "\n",
    "# Generate a new image from the combined latent representation\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "scheduler.set_timesteps(50)\n",
    "timesteps = scheduler.timesteps\n",
    "\n",
    "# Use text embeddings as encoder_hidden_states\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "prompt = \"A futuristic cityscape\"\n",
    "text_inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "text_embeddings = text_encoder(text_inputs.input_ids.to(\"cuda\"))[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in reversed(timesteps):\n",
    "        # Predict the noise to be removed\n",
    "        model_input = combined_encoded_images\n",
    "        noise_pred = pipe.unet(model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # Update the combined encoded images using the DDIM update rule\n",
    "        alpha_t = scheduler.alphas_cumprod[t]\n",
    "        alpha_prev_t = scheduler.alphas_cumprod[t-1] if t > 0 else scheduler.alphas_cumprod[0]\n",
    "        beta_t = 1 - alpha_t\n",
    "        beta_prev_t = 1 - alpha_prev_t\n",
    "        combined_encoded_images = (\n",
    "            torch.sqrt(alpha_prev_t) * (combined_encoded_images - torch.sqrt(beta_t) * noise_pred / torch.sqrt(beta_prev_t)) \n",
    "            + torch.sqrt(1 - alpha_prev_t) * torch.randn_like(combined_encoded_images)\n",
    "        )\n",
    "\n",
    "# Decode the latent representation to get the final image\n",
    "with torch.no_grad():\n",
    "    final_image = pipe.vae.decode(combined_encoded_images / 0.18215).sample()\n",
    "\n",
    "# Convert the image to a format suitable for visualization\n",
    "final_image = (final_image / 2 + 0.5).clamp(0, 1).detach().cpu().numpy().transpose(0, 2, 3, 1)[0]\n",
    "\n",
    "# Save or display the image\n",
    "plt.imshow(final_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_ch_before = 128\n",
    "input_ch_after = 128\n",
    "W = 64\n",
    "D = 4\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "skips = [0]  # Adding skip connection at the first layer\n",
    "num_components = 5  # Number of Gaussian components\n",
    "\n",
    "model = CustomModelWithGMM(input_ch_before, input_ch_after, W, D, num_heads, num_layers, skips, num_components)\n",
    "x_before = torch.randn(32, input_ch_before)\n",
    "x_after = torch.randn(32, input_ch_after)\n",
    "weights, means, covariances = model(x_before, x_after)\n",
    "\n",
    "# Sample Gaussian means from the predicted GMM parameters\n",
    "sampled_means = model.sample_gmm(weights, means, covariances, num_samples=1)\n",
    "print(sampled_means.shape)  # Should be (batch_size, num_samples, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelWithGMM(nn.Module):\n",
    "    def __init__(self, input_ch_before, input_ch_after, W, num_heads, num_layers, skips, num_components):\n",
    "        super(CustomModelWithGMM, self).__init__()\n",
    "        self.input_ch_before = input_ch_before\n",
    "        self.input_ch_after = input_ch_after\n",
    "        self.W = W\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.skips = skips\n",
    "        self.num_components = num_components  # Number of Gaussian components in the GMM\n",
    "\n",
    "        # Define the encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=W, nhead=num_heads)\n",
    "        self.transformer_encoder_past = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.transformer_encoder_future = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Initial linear layers to match input dimensions with encoder dimensions\n",
    "        self.linear_before_to_W = nn.Linear(input_ch_before, W)\n",
    "        self.linear_after_to_W = nn.Linear(input_ch_after, W)\n",
    "\n",
    "        # Layers to predict the GMM parameters\n",
    "        self.fc1 = nn.Linear(2 * W, 512)\n",
    "        self.fc_weights = nn.Linear(512, num_components)  # Weights of the GMM\n",
    "        self.fc_means = nn.Linear(512, num_components * 3)  # Means of the GMM (3D)\n",
    "        self.fc_covariances = nn.Linear(512, num_components * 3 * 3)  # Covariances of the GMM (3x3)\n",
    "\n",
    "    def forward(self, x_before, x_after):\n",
    "        # Transform inputs to match the dimension of the encoder\n",
    "        x_before_transformed = self.linear_before_to_W(x_before)  # (batch_size, W)\n",
    "        x_after_transformed = self.linear_after_to_W(x_after)     # (batch_size, W)\n",
    "\n",
    "        # Add a sequence dimension expected by the Transformer encoder\n",
    "        x_before = x_before_transformed.unsqueeze(1)  # (batch_size, seq_len=1, W)\n",
    "        x_after = x_after_transformed.unsqueeze(1)    # (batch_size, seq_len=1, W)\n",
    "\n",
    "        # Pass through the Transformer encoders with skip connections\n",
    "        for i, layer in enumerate(self.transformer_encoder_past.layers):\n",
    "            if i in self.skips:\n",
    "                x_before = x_before + x_before_transformed.unsqueeze(1)  # Add skip connection\n",
    "            x_before = layer(x_before)\n",
    "        \n",
    "        for i, layer in enumerate(self.transformer_encoder_future.layers):\n",
    "            if i in self.skips:\n",
    "                x_after = x_after + x_after_transformed.unsqueeze(1)  # Add skip connection\n",
    "            x_after = layer(x_after)\n",
    "\n",
    "        # Remove the sequence dimension\n",
    "        encoded_before = x_before.squeeze(1)  # (batch_size, W)\n",
    "        encoded_after = x_after.squeeze(1)    # (batch_size, W)\n",
    "\n",
    "        # Concatenate encoded features\n",
    "        combined_features = torch.cat((encoded_before, encoded_after), dim=1)  # (batch_size, 2 * W)\n",
    "\n",
    "        # Predict the GMM parameters\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        \n",
    "        weights = F.softmax(self.fc_weights(x), dim=1)  # Ensure weights sum to 1\n",
    "        means = self.fc_means(x).view(-1, self.num_components, 3)  # Reshape to (batch_size, num_components, 3)\n",
    "        covariances = self.fc_covariances(x).view(-1, self.num_components, 3, 3)  # Reshape to (batch_size, num_components, 3, 3)\n",
    "\n",
    "        # Ensure positive definiteness of covariances (simplest way: diagonal covariances)\n",
    "        covariances = torch.exp(covariances)  # Exponentiate to ensure positive values\n",
    "\n",
    "        return weights, means, covariances\n",
    "\n",
    "    def sample_gmm(self, weights, means, covariances, num_samples=1):\n",
    "        batch_size, num_components, _ = means.size()\n",
    "        sampled_points = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            component = torch.multinomial(weights[b], num_samples, replacement=True)\n",
    "            chosen_means = means[b, component]\n",
    "            chosen_covariances = covariances[b, component]\n",
    "            \n",
    "            sampled_point = torch.randn(num_samples, 3).to(means.device)\n",
    "            for i in range(num_samples):\n",
    "                sampled_point[i] = torch.matmul(chosen_covariances[i], sampled_point[i]) + chosen_means[i]\n",
    "            \n",
    "            sampled_points.append(sampled_point)\n",
    "        \n",
    "        sampled_points = torch.stack(sampled_points)\n",
    "        return sampled_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def normalize_coordinates(coords, grid_size):\n",
    "    \"\"\"\n",
    "    Normalize xyz coordinates to the range [0, grid_size-1].\n",
    "    \"\"\"\n",
    "    coords = coords - coords.min(0, keepdim=True)[0]  # Shift to start from zero\n",
    "    coords = coords / coords.max(0, keepdim=True)[0]  # Normalize to [0, 1]\n",
    "    coords = coords * (grid_size - 1)  # Scale to [0, grid_size-1]\n",
    "    return coords\n",
    "\n",
    "def coords_to_grid(coords, grid_size):\n",
    "    \"\"\"\n",
    "    Convert normalized xyz coordinates to a 3D grid.\n",
    "    \"\"\"\n",
    "    batch_size = coords.size(0)\n",
    "    grid = torch.zeros(batch_size, 1, grid_size, grid_size, grid_size)\n",
    "    indices = coords.long()\n",
    "    for i in range(batch_size):\n",
    "        x, y, z = indices[i]\n",
    "        grid[i, 0, x, y, z] = 1\n",
    "    return grid\n",
    "\n",
    "# Example usage\n",
    "batch_size = 32\n",
    "grid_size = 32\n",
    "xyz_coords = torch.randn(batch_size, 3)  # Input data of shape [batch_size, 3]\n",
    "\n",
    "# Normalize and map to grid\n",
    "normalized_coords = normalize_coordinates(xyz_coords, grid_size)\n",
    "spatial_data = coords_to_grid(normalized_coords, grid_size)\n",
    "print(spatial_data.shape)  # Should be [batch_size, 1, grid_size, grid_size, grid_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def normalize_coordinates(coords):\n",
    "    \"\"\"\n",
    "    Normalize xyz coordinates to the range [0, 1].\n",
    "    \"\"\"\n",
    "    coords = coords - coords.min(0, keepdim=True)[0]  # Shift to start from zero\n",
    "    coords = coords / coords.max(0, keepdim=True)[0]  # Normalize to [0, 1]\n",
    "    return coords\n",
    "\n",
    "def coords_to_grid(coords, grid_size):\n",
    "    \"\"\"\n",
    "    Convert normalized xyz coordinates to a 3D grid.\n",
    "    \"\"\"\n",
    "    batch_size = coords.size(0)\n",
    "    grid = torch.zeros(batch_size, 1, grid_size, grid_size, grid_size)\n",
    "    indices = (coords * (grid_size - 1)).long()\n",
    "    for i in range(batch_size):\n",
    "        x, y, z = indices[i]\n",
    "        grid[i, 0, x, y, z] = 1\n",
    "    return grid\n",
    "\n",
    "class CustomModelWithGMM(nn.Module):\n",
    "    def __init__(self, input_ch_before, input_ch_after, grid_size, hidden_dim, num_layers, num_components):\n",
    "        super(CustomModelWithGMM, self).__init__()\n",
    "        self.input_ch_before = input_ch_before\n",
    "        self.input_ch_after = input_ch_after\n",
    "        self.grid_size = grid_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_components = num_components  # Number of Gaussian components in the GMM\n",
    "\n",
    "        # Define 3D convolutional layers to capture spatial features\n",
    "        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Define GRU layer to capture temporal dependencies\n",
    "        self.gru = nn.GRU(256 * grid_size * grid_size * grid_size, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Layers to predict the GMM parameters\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc_weights = nn.Linear(hidden_dim, num_components)  # Weights of the GMM\n",
    "        self.fc_means = nn.Linear(hidden_dim, num_components * 3)  # Means of the GMM (3D)\n",
    "        self.fc_covariances = nn.Linear(hidden_dim, num_components * 3 * 3)  # Covariances of the GMM (3x3)\n",
    "\n",
    "    def forward(self, x_before, x_after):\n",
    "        # Normalize and map to grid\n",
    "        x_before_normalized = normalize_coordinates(x_before)\n",
    "        x_after_normalized = normalize_coordinates(x_after)\n",
    "        x_before_grid = coords_to_grid(x_before_normalized, self.grid_size)\n",
    "        x_after_grid = coords_to_grid(x_after_normalized, self.grid_size)\n",
    "\n",
    "        # Concatenate the key frames along the channel dimension\n",
    "        x = torch.cat([x_before_grid, x_after_grid], dim=1)  # (batch_size, 2, grid_size, grid_size, grid_size)\n",
    "\n",
    "        # Apply 3D convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and prepare for GRU\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # (batch_size, 256 * grid_size * grid_size * grid_size)\n",
    "\n",
    "        # Apply GRU\n",
    "        x_combined = torch.stack((x, x), dim=1)  # (batch_size, 2, flattened_size)\n",
    "        gru_output, _ = self.gru(x_combined)  # (batch_size, 2, hidden_dim)\n",
    "\n",
    "        # Flatten the GRU output\n",
    "        gru_output_flattened = gru_output.reshape(gru_output.size(0), -1)  # (batch_size, 2 * hidden_dim)\n",
    "\n",
    "        # Predict the GMM parameters\n",
    "        x = F.relu(self.fc1(gru_output_flattened))\n",
    "        \n",
    "        weights = F.softmax(self.fc_weights(x), dim=1)  # Ensure weights sum to 1\n",
    "        means = self.fc_means(x).view(-1, self.num_components, 3)  # Reshape to (batch_size, num_components, 3)\n",
    "        covariances = self.fc_covariances(x).view(-1, self.num_components, 3, 3)  # Reshape to (batch_size, num_components, 3, 3)\n",
    "\n",
    "        # Ensure positive definiteness of covariances (simplest way: diagonal covariances)\n",
    "        covariances = torch.exp(covariances)  # Exponentiate to ensure positive values\n",
    "\n",
    "        return weights, means, covariances\n",
    "\n",
    "    def sample_gmm(self, weights, means, covariances, num_samples=1):\n",
    "        batch_size, num_components, _ = means.size()\n",
    "        sampled_points = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            component = torch.multinomial(weights[b], num_samples, replacement=True)\n",
    "            chosen_means = means[b, component]\n",
    "            chosen_covariances = covariances[b, component]\n",
    "            \n",
    "            sampled_point = torch.randn(num_samples, 3).to(means.device)\n",
    "            for i in range(num_samples):\n",
    "                sampled_point[i] = torch.matmul(chosen_covariances[i], sampled_point[i]) + chosen_means[i]\n",
    "            \n",
    "            sampled_points.append(sampled_point)\n",
    "        \n",
    "        sampled_points = torch.stack(sampled_points)\n",
    "        return sampled_points\n",
    "\n",
    "# Example usage\n",
    "input_ch_before = 3\n",
    "input_ch_after = 3\n",
    "grid_size = 32\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_components = 5  # Number of Gaussian components\n",
    "\n",
    "model = CustomModelWithGMM(input_ch_before, input_ch_after, grid_size, hidden_dim, num_layers, num_components)\n",
    "x_before = torch.randn(32, input_ch_before)  # (batch_size, input_ch_before)\n",
    "x_after = torch.randn(32, input_ch_after)    # (batch_size, input_ch_after)\n",
    "\n",
    "weights, means, covariances = model(x_before, x_after)\n",
    "\n",
    "# Sample Gaussian means from the predicted GMM parameters\n",
    "sampled_means = model.sample_gmm(weights, means, covariances, num_samples=10)\n",
    "print(sampled_means.shape)  # Should be (batch_size, num_samples, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def normalize_coordinates(coords):\n",
    "    \"\"\"\n",
    "    Normalize xyz coordinates to the range [0, 1].\n",
    "    \"\"\"\n",
    "    coords = coords - coords.min(0, keepdim=True)[0]  # Shift to start from zero\n",
    "    coords = coords / coords.max(0, keepdim=True)[0]  # Normalize to [0, 1]\n",
    "    return coords\n",
    "\n",
    "def coords_to_grid(coords, grid_size):\n",
    "    \"\"\"\n",
    "    Convert normalized xyz coordinates to a 3D grid.\n",
    "    \"\"\"\n",
    "    batch_size = coords.size(0)\n",
    "    grid = torch.zeros(batch_size, 1, grid_size, grid_size, grid_size)\n",
    "    indices = (coords * (grid_size - 1)).long()\n",
    "    for i in range(batch_size):\n",
    "        x, y, z = indices[i]\n",
    "        grid[i, 0, x, y, z] = 1\n",
    "    return grid\n",
    "\n",
    "class CustomModelWithGMM(nn.Module):\n",
    "    def __init__(self, input_ch_before, input_ch_after, grid_size, hidden_dim, num_layers, num_components):\n",
    "        super(CustomModelWithGMM, self).__init__()\n",
    "        self.input_ch_before = input_ch_before\n",
    "        self.input_ch_after = input_ch_after\n",
    "        self.grid_size = grid_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_components = num_components  # Number of Gaussian components in the GMM\n",
    "\n",
    "        # Define 3D convolutional layers to capture spatial features\n",
    "        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Define GRU layer to capture temporal dependencies\n",
    "        self.gru = nn.GRU(256 * grid_size * grid_size * grid_size, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Layers to predict the GMM parameters\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc_weights = nn.Linear(hidden_dim, num_components)  # Weights of the GMM\n",
    "        self.fc_means = nn.Linear(hidden_dim, num_components * 3)  # Means of the GMM (3D)\n",
    "        self.fc_covariances = nn.Linear(hidden_dim, num_components * 3 * 3)  # Covariances of the GMM (3x3)\n",
    "\n",
    "    def forward(self, x_before, x_after):\n",
    "        # Normalize and map to grid\n",
    "        x_before_normalized = normalize_coordinates(x_before)\n",
    "        x_after_normalized = normalize_coordinates(x_after)\n",
    "        x_before_grid = coords_to_grid(x_before_normalized, self.grid_size)\n",
    "        x_after_grid = coords_to_grid(x_after_normalized, self.grid_size)\n",
    "\n",
    "        # Concatenate the key frames along the channel dimension\n",
    "        x = torch.cat([x_before_grid, x_after_grid], dim=1)  # (batch_size, 2, grid_size, grid_size, grid_size)\n",
    "\n",
    "        # Apply 3D convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and prepare for GRU\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # (batch_size, 256 * grid_size * grid_size * grid_size)\n",
    "\n",
    "        # Apply GRU\n",
    "        x_combined = torch.stack((x, x), dim=1)  # (batch_size, 2, flattened_size)\n",
    "        gru_output, _ = self.gru(x_combined)  # (batch_size, 2, hidden_dim)\n",
    "\n",
    "        # Flatten the GRU output\n",
    "        gru_output_flattened = gru_output.reshape(gru_output.size(0), -1)  # (batch_size, 2 * hidden_dim)\n",
    "\n",
    "        # Predict the GMM parameters\n",
    "        x = F.relu(self.fc1(gru_output_flattened))\n",
    "        \n",
    "        weights = F.softmax(self.fc_weights(x), dim=1)  # Ensure weights sum to 1\n",
    "        means = self.fc_means(x).view(-1, self.num_components, 3)  # Reshape to (batch_size, num_components, 3)\n",
    "        covariances = self.fc_covariances(x).view(-1, self.num_components, 3, 3)  # Reshape to (batch_size, num_components, 3, 3)\n",
    "\n",
    "        # Ensure positive definiteness of covariances (simplest way: diagonal covariances)\n",
    "        covariances = torch.exp(covariances)  # Exponentiate to ensure positive values\n",
    "\n",
    "        return weights, means, covariances\n",
    "\n",
    "    def sample_gmm(self, weights, means, covariances, num_samples=1):\n",
    "        batch_size, num_components, _ = means.size()\n",
    "        sampled_points = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            component = torch.multinomial(weights[b], num_samples, replacement=True)\n",
    "            chosen_means = means[b, component]\n",
    "            chosen_covariances = covariances[b, component]\n",
    "            \n",
    "            sampled_point = torch.randn(num_samples, 3).to(means.device)\n",
    "            for i in range(num_samples):\n",
    "                sampled_point[i] = torch.matmul(chosen_covariances[i], sampled_point[i]) + chosen_means[i]\n",
    "            \n",
    "            sampled_points.append(sampled_point)\n",
    "        \n",
    "        sampled_points = torch.stack(sampled_points)\n",
    "        return sampled_points\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_ch_before = 3\n",
    "input_ch_after = 3\n",
    "grid_size = 32\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_components = 5  # Number of Gaussian components\n",
    "\n",
    "model = CustomModelWithGMM(input_ch_before, input_ch_after, grid_size, hidden_dim, num_layers, num_components).to(device)\n",
    "x_before = torch.randn(32, input_ch_before).to(device)  # (batch_size, input_ch_before)\n",
    "x_after = torch.randn(32, input_ch_after).to(device)    # (batch_size, input_ch_after)\n",
    "\n",
    "weights, means, covariances = model(x_before, x_after)\n",
    "\n",
    "# Sample Gaussian means from the predicted GMM parameters\n",
    "sampled_means = model.sample_gmm(weights, means, covariances, num_samples=10)\n",
    "print(sampled_means.shape)  # Should be (batch_size, num_samples, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention3D(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads):\n",
    "        super(SelfAttention3D, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = latent_dim // num_heads\n",
    "        assert self.head_dim * num_heads == latent_dim, \"latent_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(latent_dim, latent_dim)\n",
    "        self.key = nn.Linear(latent_dim, latent_dim)\n",
    "        self.value = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_out = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # x shape: [batch_size, 2, latent_dim]\n",
    "        # Split into query, key, value projections\n",
    "        Q = self.query(x)  # [batch_size, 2, latent_dim]\n",
    "        K = self.key(x)  # [batch_size, 2, latent_dim]\n",
    "        V = self.value(x)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        K = K.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        V = V.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        energy = torch.einsum(\"bnqd,bnkd->bnqk\", [Q, K]) / self.scale  # [batch_size, num_heads, 2, 2]\n",
    "        attention = torch.softmax(energy, dim=-1)  # [batch_size, num_heads, 2, 2]\n",
    "        \n",
    "        # Compute the attended values\n",
    "        out = torch.einsum(\"bnqk,bnvd->bnqd\", [attention, V])  # [batch_size, num_heads, 2, head_dim]\n",
    "        \n",
    "        # Reshape and combine heads\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()  # [batch_size, 2, num_heads, head_dim]\n",
    "        out = out.view(batch_size, 2, self.num_heads * self.head_dim)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim*2, hidden_dim*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.fc_mu = nn.Linear(hidden_dim*4 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim*4 * 4 * 4, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(2 * latent_dim, latent_dim * 16 * 16)  # Flatten the input and expand\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dim, hidden_dim*2, kernel_size=4, stride=2, padding=1)  # Output: [8, 64, 32, 32]\n",
    "        self.deconv2 = nn.ConvTranspose2d(hidden_dim*2, hidden_dim, kernel_size=4, stride=2, padding=1)   # Output: [8, 32, 64, 64]\n",
    "        self.deconv3 = nn.ConvTranspose2d(hidden_dim, hidden_dim // 2, kernel_size=4, stride=2, padding=1)   # Output: [8, 16, 128, 128]\n",
    "        self.deconv4 = nn.ConvTranspose2d(hidden_dim // 2, output_channels, kernel_size=1)                        # Output: [8, 3, 128, 128]\n",
    "        \n",
    "    def forward(self, x,z_w,z_h):\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [8, 2, 128] -> [8, 2 * 128]\n",
    "        x = F.relu(self.fc(x))  # Fully connected: [8, 2 * 128] -> [8, 128 * 16 * 16]\n",
    "        x = x.view(x.size(0), 128, 16, 16)  # Reshape to 2D: [8, 128 * 16 * 16] -> [8, 128, 16, 16]\n",
    "        x = F.relu(self.deconv1(x))  # Transposed conv: [8, 128, 16, 16] -> [8, 64, 32, 32]\n",
    "        x = F.relu(self.deconv2(x))  # Transposed conv: [8, 64, 32, 32] -> [8, 32, 64, 64]\n",
    "        x = F.relu(self.deconv3(x))  # Transposed conv: [8, 32, 64, 64] -> [8, 16, 128, 128]\n",
    "        x = torch.sigmoid(self.deconv4(x))  # Transposed conv: [8, 16, 128, 128] -> [8, 3, 128, 128]\n",
    "        x = F.interpolate(x, size=(z_w, z_h), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, latent_dim, output_channels, num_heads):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder_before = Encoder(input_channels, hidden_dim, latent_dim)\n",
    "        self.encoder_after = Encoder(input_channels, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, output_channels)\n",
    "        self.SelfAttention3D = SelfAttention3D(latent_dim, num_heads)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x_before, x_after):\n",
    "        z_w = x_before.size()[2]\n",
    "        z_h = x_before.size()[3]\n",
    "        mu_before, logvar_before = self.encoder_before(x_before)\n",
    "        z_before = self.reparameterize(mu_before, logvar_before) # -----> [batch size, latent dim]\n",
    "        mu_after, logvar_after = self.encoder_after(x_after)\n",
    "        z_after = self.reparameterize(mu_after, logvar_after) # -----> [batch size, latent dim]\n",
    "        # add frame dim\n",
    "        z_before = z_before.unsqueeze(1)\n",
    "        z_after = z_after.unsqueeze(1)\n",
    "        print(z_before.shape)\n",
    "        # concat\n",
    "        z = torch.cat([z_before,z_after],dim=1)\n",
    "        z = self.SelfAttention3D(z)\n",
    "        #print(z.size())\n",
    "        #print(z.size())\n",
    "        return self.decoder(z,z_w,z_h)\n",
    "\n",
    "# Example usage\n",
    "input_channels = 3\n",
    "hidden_dim = 32\n",
    "latent_dim = 128\n",
    "output_channels = 3\n",
    "num_heads = 8\n",
    "\n",
    "model = VAE(input_channels, hidden_dim, latent_dim, output_channels, num_heads).cuda()\n",
    "\n",
    "# Dummy data with different sizes\n",
    "input_data1 = torch.randn(8, 3, 240, 480).cuda()  # batch of 8, 64x64 RGB images\n",
    "input_data2 = torch.randn(8, 3, 240, 480).cuda()  # batch of 8, 128x128 RGB images\n",
    "\n",
    "output1 = model(input_data1, input_data2)\n",
    "\n",
    "print(output1.shape)  # should match input_data1 shape (8, 3, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention3D(nn.Module):\n",
    "    def __init__(self, latent_dim, num_heads):\n",
    "        super(SelfAttention3D, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = latent_dim // num_heads\n",
    "        assert self.head_dim * num_heads == latent_dim, \"latent_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(latent_dim, latent_dim)\n",
    "        self.key = nn.Linear(latent_dim, latent_dim)\n",
    "        self.value = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_out = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])) #.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # x shape: [batch_size, 2, latent_dim]\n",
    "        # Split into query, key, value projections\n",
    "        Q = self.query(x)  # [batch_size, 2, latent_dim]\n",
    "        K = self.key(x)  # [batch_size, 2, latent_dim]\n",
    "        V = self.value(x)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        K = K.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        V = V.view(batch_size, 2, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, 2, head_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        energy = torch.einsum(\"bnqd,bnkd->bnqk\", [Q, K]) / self.scale  # [batch_size, num_heads, 2, 2]\n",
    "        attention = torch.softmax(energy, dim=-1)  # [batch_size, num_heads, 2, 2]\n",
    "        \n",
    "        # Compute the attended values\n",
    "        out = torch.einsum(\"bnqk,bnvd->bnqd\", [attention, V])  # [batch_size, num_heads, 2, head_dim]\n",
    "        \n",
    "        # Reshape and combine heads\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()  # [batch_size, 2, num_heads, head_dim]\n",
    "        out = out.view(batch_size, 2, self.num_heads * self.head_dim)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)  # [batch_size, 2, latent_dim]\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "batch_size = 8\n",
    "latent_dim = 128\n",
    "num_heads = 8\n",
    "\n",
    "self_attention = SelfAttention3D(latent_dim, num_heads)\n",
    "input_tensor = torch.randn(batch_size, 2, latent_dim)\n",
    "\n",
    "output_tensor = self_attention(input_tensor)\n",
    "print(output_tensor.shape)  # should be [batch_size, 2, latent_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = torch.randn(1, 128)\n",
    "latent_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeats = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_features_before = latent_data.repeat(num_repeats + 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_features_before.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the image\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "init_image = torch.randn(3, 270, 480)\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt, image=init_image)\n",
    "\n",
    "image = image.images[0]\n",
    "\n",
    "# Define a transform to convert the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Apply the transform to the image\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Print the shape of the tensor and the tensor itself\n",
    "print(f\"Image tensor shape: {image_tensor.shape}\")\n",
    "print(image_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pixel_values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = Image.open(\"/home/ad/20813716/Deformable-3D-Gaussians/output/exp_nerfds_press-2/train/ours_40000/renders/00000.png\")\n",
    "init_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad/20813716/miniconda3/envs/defou3d_ver1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level embeddings shape: torch.Size([1, 50, 768])\n",
      "Pooled image embeddings shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the CLIP processor and model\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Open an image file\n",
    "image_ref_1 = torch.randn(3,244) #Image.open('/home/ad/20813716/Deformable-3D-Gaussians/output/exp_nerfds_press-2/train/ours_40000/renders/00000.png')\n",
    "image_ref_2 = torch.randn(3,244,244) #Image.open('/home/ad/20813716/Deformable-3D-Gaussians/output/exp_nerfds_press-2/train/ours_40000/renders/00005.png')\n",
    "image_ref_1 = image_ref_1.unsqueeze(0)\n",
    "image_ref_2 = image_ref_2.unsqueeze(0)\n",
    "# Process the image\n",
    "#input_1 = processor(images=image_ref_1, return_tensors=\"pt\")\n",
    "#input_2 = processor(images=image_ref_2, return_tensors=\"pt\")\n",
    "#print(input_1)\n",
    "#print(input_2)\n",
    "\n",
    "# Get image embeddings and pooled output\n",
    "with torch.no_grad():\n",
    "    # Image 1\n",
    "    output_1 = model.vision_model(image_ref_1) #(**input_1)\n",
    "    image_embedding_1 = output_1.last_hidden_state  # Token-level embeddings\n",
    "    pooled_image_embed_1 = output_1.pooler_output  # Pooled embedding\n",
    "    # Image 2\n",
    "    #output_2 = model.vision_model(image_ref_2) #(**input_2)\n",
    "    #image_embedding_2 = output_2.last_hidden_state  # Token-level embeddings\n",
    "    #pooled_image_embed_2 = output_2.pooler_output  # Pooled embedding\n",
    "\n",
    "# Print the shapes of the embeddings\n",
    "print(f\"Token-level embeddings shape: {image_embedding_1.shape}\")\n",
    "print(f\"Pooled image embeddings shape: {pooled_image_embed_1.shape}\")\n",
    "#print(f\"Token-level embeddings shape: {image_embedding_2.shape}\")\n",
    "#print(f\"Pooled image embeddings shape: {pooled_image_embed_2.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_1['pixel_values'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONDITIONING ON 2 IMAGES\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image_embedding = torch.cat([image_embedding_1,image_embedding_2],dim=1)\n",
    "print(image_embedding.shape)\n",
    "interpolated_tensor = F.interpolate(image_embedding.permute(0, 2, 1), size=77, mode='linear', align_corners=True).permute(0, 2, 1)  # Shape: [1, 77, 768]\n",
    "print(interpolated_tensor.shape)\n",
    "# Step 3: Linear layer to transform the feature dimension from 768 to 1280\n",
    "linear_layer = torch.nn.Linear(768, 1280)\n",
    "final_image_embedding = linear_layer(interpolated_tensor)  # Shape: [1, 77, 1280]\n",
    "\n",
    "# Check the shape of the final tensor\n",
    "print(final_image_embedding.shape)  # Output should be torch.Size([1, 77, 1280])\n",
    "\n",
    "\n",
    "pooled_image_embedding = torch.cat([pooled_image_embed_1,pooled_image_embed_2],dim=1)\n",
    "\n",
    "linear_pooled_prompt = torch.nn.Linear(1536, 1280)\n",
    "\n",
    "final_pooled_image_embedding = linear_pooled_prompt(pooled_image_embedding)\n",
    "\n",
    "print(final_pooled_image_embedding.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "### ONLY CONDITIONING ON 1 IMAGE\n",
    "## final_image_embedding\n",
    "final_image_embedding = torch.nn.functional.pad(image_embedding_1, (0, 0, 0, 27))  \n",
    "linear_layer = nn.Linear(768, 1280)\n",
    "# Apply the linear layer to transform the feature dimension\n",
    "final_image_embedding = linear_layer(final_image_embedding)\n",
    "\n",
    "## final_pooled_image_embedding\n",
    "pooled_linear_layer = nn.Linear(768, 1280)\n",
    "final_pooled_image_embedding = pooled_linear_layer(pooled_image_embed_1)\n",
    "\n",
    "print(final_image_embedding.size())\n",
    "print(final_pooled_image_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "final_image_embedding = final_image_embedding.to(\"cuda\")\n",
    "final_pooled_image_embedding = final_pooled_image_embedding.to(\"cuda\")\n",
    "\n",
    "init_image = Image.open('/home/ad/20813716/Deformable-3D-Gaussians/output/exp_nerfds_press-2/train/ours_40000/renders/00000.png').convert(\"RGB\")\n",
    "\n",
    "image = pipe(prompt_embeds = final_image_embedding, pooled_prompt_embeds = final_pooled_image_embedding,  image=init_image, output_type=\"latent\",num_inference_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "url = \"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png\"\n",
    "\n",
    "init_image = load_image(url).convert(\"RGB\")\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt, image=init_image, output_type=\"latent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "init_image = torch.randn(3, 270, 480) #Image.open('/home/ad/20813716/Deformable-3D-Gaussians/output/exp_nerfds_press-2/train/ours_40000/renders/00000.png').convert(\"RGB\")\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt, image=init_image, output_type=\"latent\",num_inference_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.images[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = image.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tensor to 1D\n",
    "flattened_tensor = latent_data.flatten()\n",
    "\n",
    "# Calculate the number of elements needed for the target shape\n",
    "num_elements_needed = 50000 * 64\n",
    "\n",
    "# If the flattened tensor has fewer elements than needed, pad with zeros (or replicate)\n",
    "if flattened_tensor.numel() < num_elements_needed:\n",
    "    repeats = (num_elements_needed // flattened_tensor.numel()) + 1\n",
    "    padded_tensor = flattened_tensor.repeat(repeats)[:num_elements_needed]\n",
    "else:\n",
    "    padded_tensor = flattened_tensor[:num_elements_needed]\n",
    "\n",
    "# Reshape to the desired shape [50000, 64]\n",
    "reshaped_tensor = padded_tensor.reshape(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize a dictionary to store tensors\n",
    "tensor_dict = {}\n",
    "\n",
    "# Iterate and store tensors\n",
    "for i in range(10):  # Example loop for 10 iterations\n",
    "    # Generate a tensor (e.g., a random tensor for this example)\n",
    "    tensor = torch.rand(3, 3)  # 3x3 tensor with random values\n",
    "    \n",
    "    # Save the tensor in the dictionary with a unique key\n",
    "    tensor_dict[f'tensor_{i}'] = tensor\n",
    "\n",
    "# Accessing stored tensors\n",
    "for key, tensor in tensor_dict.items():\n",
    "    print(f\"{key}: {tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict['tensor_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(4 * 33 * 60, 1024 * 8 * 8)  # Reduced size\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),  # (512, 16, 16)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),   # (256, 32, 32)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),   # (128, 64, 64)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),    # (64, 128, 128)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),     # (32, 256, 256)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),     # (16, 512, 512)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),      # (8, 1024, 1024)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),       # (1, 2048, 2048)\n",
    "        )\n",
    "        self.final_fc = nn.Linear(2048 * 2048, 50000 * 64)  # Adjusted size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the latent vector\n",
    "        x = self.fc(x)             # Fully connected layer\n",
    "        x = x.view(x.size(0), 1024, 8, 8)  # Reshape to (batch_size, 1024, 8, 8)\n",
    "        x = self.deconv_layers(x)  # Pass through the transposed convolutional layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten again\n",
    "        x = self.final_fc(x)       # Final fully connected layer to output size\n",
    "        x = x.view(-1, 50000, 64)  # Reshape to desired output size\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "latent = torch.randn(1, 4, 33, 60)  # Batch size of 1 for simplicity\n",
    "decoder = VAEDecoder()\n",
    "output = decoder(latent)\n",
    "print(output.shape)  # Should print torch.Size([1, 50000, 64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 33, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention3D, self).__init__()\n",
    "        self.query_conv = nn.Conv3d(in_channels, max(1, in_channels // 8), kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels, max(1, in_channels // 8), kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, D, H, W = x.size()\n",
    "        query = self.query_conv(x).view(batch_size, -1, D * H * W).permute(0, 2, 1)  # B, D*H*W, C//8\n",
    "        key = self.key_conv(x).view(batch_size, -1, D * H * W)  # B, C//8, D*H*W\n",
    "        value = self.value_conv(x).view(batch_size, -1, D * H * W)  # B, C, D*H*W\n",
    "\n",
    "        attention = torch.bmm(query, key)  # B, D*H*W, D*H*W\n",
    "        attention = F.softmax(attention, dim=-1)  # B, D*H*W, D*H*W\n",
    "\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))  # B, C, D*H*W\n",
    "        out = out.view(batch_size, C, D, H, W)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ConvDecoderSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvDecoderSelfAttention, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv3d(in_channels, in_channels, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.self_attention = SelfAttention3D(in_channels)\n",
    "        self.final_conv = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d((1, 33, 60))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)  # Add an additional dimension for depth: [B, C, 1, H, W]\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.global_pool(x)  # Aggregate along the batch dimension\n",
    "        return x.mean(dim=0, keepdim=True).squeeze(2).squeeze(0) # Reduce batch dimension to 1 and remove depth dimension\n",
    "\n",
    "# Example tensor with size [100, 4, 33, 60]\n",
    "input_tensor = torch.randn(100, 4, 33, 60)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConvDecoderSelfAttention(in_channels=4)\n",
    "\n",
    "# Apply the model to the input tensor\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([1, 4, 33, 60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.randn(100, 4, 33, 60)\n",
    "input_tensor.ndimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.randn(10000,1)\n",
    "input_tensor[1,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 1280])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CameraPoseEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim=37, output_dim=1280):\n",
    "        super(CameraPoseEmbedding, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, camera_center, world_view_transform, full_proj_transform, fovx, fovy):\n",
    "        # Flatten the [4, 4] tensors to [16]\n",
    "        world_view_flat = world_view_transform.view(-1)  # [16]\n",
    "        proj_transform_flat = full_proj_transform.view(-1)  # [16]\n",
    "\n",
    "        # Concatenate all the components\n",
    "        pose_vector = torch.cat([\n",
    "            camera_center,              # [3]\n",
    "            world_view_flat,            # [16]\n",
    "            proj_transform_flat,        # [16]\n",
    "            fovx.view(-1),              # [1]\n",
    "            fovy.view(-1)               # [1]\n",
    "        ], dim=-1)  # [37]\n",
    "\n",
    "        # Map to the desired size [1280]\n",
    "        embedded_pose = self.fc(pose_vector)  # [1280]\n",
    "\n",
    "        # Repeat or reshape to match [1, 77, 1280]\n",
    "        embedded_pose = embedded_pose.unsqueeze(0).repeat(77, 1)  # [77, 1280]\n",
    "\n",
    "        # Add batch dimension [1, 77, 1280]\n",
    "        embedded_pose = embedded_pose.unsqueeze(0)  # [1, 77, 1280]\n",
    "\n",
    "        return embedded_pose\n",
    "\n",
    "# Example usage\n",
    "camera_center = torch.randn(3)\n",
    "world_view_transform = torch.randn(4, 4)\n",
    "full_proj_transform = torch.randn(4, 4)\n",
    "fovx = 1.0 #torch.tensor(1.0)\n",
    "fovy = 2.0 #torch.tensor(1.0)\n",
    "fovx = torch.tensor(fovx)\n",
    "fovy = torch.tensor(fovy)\n",
    "\n",
    "\n",
    "model = CameraPoseEmbedding()\n",
    "output = model(camera_center, world_view_transform, full_proj_transform, fovx, fovy)\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([1, 77, 1280])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1280])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CameraPoseEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim=37, output_dim=1280):\n",
    "        super(CameraPoseEmbedding, self).__init__()\n",
    "        # Linear layer to map from input_dim to output_dim\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, camera_center, world_view_transform, full_proj_transform, fovx, fovy):\n",
    "        # Flatten the [4, 4] tensors to [16]\n",
    "        world_view_flat = world_view_transform.view(-1)  # [16]\n",
    "        proj_transform_flat = full_proj_transform.view(-1)  # [16]\n",
    "\n",
    "        # Convert FoVx and FoVy to tensors if they are floats\n",
    "        fovx_tensor = torch.tensor([fovx], dtype=torch.float32)\n",
    "        fovy_tensor = torch.tensor([fovy], dtype=torch.float32)\n",
    "\n",
    "        # Concatenate all components into a single vector\n",
    "        pose_vector = torch.cat([\n",
    "            camera_center,              # [3]\n",
    "            world_view_flat,            # [16]\n",
    "            proj_transform_flat,        # [16]\n",
    "            fovx_tensor,                # [1]\n",
    "            fovy_tensor                 # [1]\n",
    "        ], dim=-1)  # [37]\n",
    "\n",
    "        # Map to the desired size [1280]\n",
    "        embedded_pose = self.fc(pose_vector)  # [1280]\n",
    "\n",
    "        # Add batch dimension [1, 1280]\n",
    "        embedded_pose = embedded_pose.unsqueeze(0)  # [1, 1280]\n",
    "\n",
    "        return embedded_pose\n",
    "\n",
    "# Example usage\n",
    "camera_center = torch.randn(3)\n",
    "world_view_transform = torch.randn(4, 4)\n",
    "full_proj_transform = torch.randn(4, 4)\n",
    "fovx = 1.0  # Example FoVx as a float\n",
    "fovy = 1.0  # Example FoVy as a float\n",
    "\n",
    "print(world_view_transform.size())\n",
    "print(full_proj_transform.size())\n",
    "\n",
    "model = CameraPoseEmbedding()\n",
    "output = model(camera_center, world_view_transform, full_proj_transform, fovx, fovy)\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([1, 1280])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defou3d_ver1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
